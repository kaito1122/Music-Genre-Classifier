{"cells":[{"cell_type":"code","execution_count":1,"id":"e1e70d0c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27334,"status":"ok","timestamp":1732237897096,"user":{"displayName":"Kaito Minami","userId":"17520602151836041630"},"user_tz":300},"id":"e1e70d0c","outputId":"32446976-0676-434b-afc3-0747a0088c13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n","Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (1.7.0)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from PyWavelets) (1.26.4)\n"]}],"source":["!pip install Pydub\n","!pip install PyWavelets\n","# !pip install openl3\n","import numpy as np\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import pywt\n","import pandas as pd\n","import os\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","import audioread\n","from pydub import AudioSegment\n","import tensorflow as tf\n","#from tensorflow.keras import layers, models\n","#import openl3\n","import audioread\n","import tensorflow_hub as hub\n","from keras import layers, models, preprocessing\n","import keras.backend as K\n","from keras.preprocessing.sequence import pad_sequences\n","from keras import mixed_precision\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n","import math\n","import random\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"id":"X_iyNvV1qrVx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2083,"status":"ok","timestamp":1732237899170,"user":{"displayName":"Kaito Minami","userId":"17520602151836041630"},"user_tz":300},"id":"X_iyNvV1qrVx","outputId":"4eb7b5da-f15e-40fd-cd6d-72320d9f10be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"b93cf69e","metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1732237899171,"user":{"displayName":"Kaito Minami","userId":"17520602151836041630"},"user_tz":300},"id":"b93cf69e"},"outputs":[],"source":["# Load the datasets\n","#file_30_sec = 'data/GTZAN/features_30_sec.csv'\n","#file_3_sec = 'data/GTZAN/features_3_sec.csv'\n","\n","file_30_sec = '/content/drive/MyDrive/Colab Notebooks/GTZAN/features_30_sec.csv'\n","file_3_sec = '/content/drive/MyDrive/Colab Notebooks/GTZAN/features_3_sec.csv'\n","audio_directory = '/content/drive/MyDrive/Colab Notebooks/GTZAN/genres_original'\n","\n","df_30_sec = pd.read_csv(file_30_sec)\n","df_3_sec = pd.read_csv(file_3_sec)"]},{"cell_type":"code","execution_count":4,"id":"b0ec6226","metadata":{"executionInfo":{"elapsed":2513,"status":"ok","timestamp":1732237901672,"user":{"displayName":"Kaito Minami","userId":"17520602151836041630"},"user_tz":300},"id":"b0ec6226"},"outputs":[],"source":["# Load VGGish model from TensorFlow Hub\n","vggish_model = hub.load('https://tfhub.dev/google/vggish/1')\n","\n","# Function to extract VGGish features with correct preprocessing\n","def extract_vggish_features(y, sr, file_path):\n","    try:\n","        # Resample the audio to 16 kHz for VGGish input\n","        y_resampled = librosa.resample(y, orig_sr=sr, target_sr=16000)\n","\n","        # Ensure input is in float32 format\n","        y_resampled = np.array(y_resampled, dtype=np.float32)\n","\n","        # VGGish expects an input with shape [num_samples], no batch dimension\n","        # Convert to tensor for the model\n","        y_tensor = tf.convert_to_tensor(y_resampled, dtype=tf.float32)\n","\n","        # Extract features using VGGish model\n","        vggish_features = vggish_model(y_tensor)\n","\n","        # If output is a tensor, convert it to numpy array\n","        if isinstance(vggish_features, tf.Tensor):\n","            vggish_features = vggish_features.numpy()\n","\n","        # Flatten the features to use in ML models\n","        vggish_features_flattened = vggish_features.flatten()\n","    except Exception as e:\n","        print(f\"Error extracting VGGish features from {file_path}: {e}\")\n","        vggish_features_flattened = None\n","\n","    return vggish_features_flattened\n","\n","def extract_features(file_path, sr=22050, n_mels=128, wavelet='db1'):\n","    try:\n","        # Load audio file\n","        y, sr = librosa.load(file_path, sr=sr)\n","        # Ensure input is in float32 format\n","        y = np.array(y, dtype=np.float32)\n","\n","        # Extract Mel spectrogram\n","        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n","        mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n","        mel_spectrogram_flattened = mel_spectrogram_db.flatten()  # Flatten for use in ML models\n","\n","        # Extract wavelet features\n","        coeffs = pywt.wavedec(y, wavelet, level=5)\n","        wavelet_features = np.concatenate([np.array(c).flatten() for c in coeffs])\n","\n","        # Extract VGGish features\n","        vggish_features_flattened = extract_vggish_features(y, sr, file_path)\n","\n","        # Extract waveform features\n","        waveform = y\n","        if waveform.shape[0] % sr != 0:\n","            waveform = np.concatenate([waveform, np.zeros(sr)])\n","        inp = tf.constant( np.array([waveform]) , dtype='float32'  )\n","\n","        # Combine all features independently into a dictionary (without chroma)\n","        features = {\n","            'mel_spectrogram': mel_spectrogram_flattened,\n","            'wavelet': wavelet_features,\n","            'vggish': vggish_features_flattened if vggish_features_flattened is not None else np.array([]),\n","            'waveform': waveform\n","        }\n","    except Exception as e:\n","        print(f\"Error processing audio file {file_path}: {e}\")\n","        features = {\n","            'mel_spectrogram': np.array([]),\n","            'wavelet': np.array([]),\n","            'vggish': np.array([])\n","        }\n","\n","    return features\n","\n","# Function to process the entire dataset and extract features\n","# and combine them with existing CSV features\n","def process_dataset(df, audio_directory):\n","    features = {\n","        'mel_spectrogram': [],\n","        'wavelet': [],\n","        'vggish': [],\n","        'waveform': [],\n","        'mfcc': [],\n","        'chroma': [],\n","        'rms': [],\n","        'spectral_centroid': [],\n","        'spectral_bandwidth': [],\n","        'zero_crossing_rate': [],\n","        'tempo': []\n","    }\n","    labels = []\n","\n","    for idx, row in df.iterrows():\n","        file_path = os.path.join(audio_directory, row['label'], row['filename'])\n","        extracted_features = extract_features(file_path)\n","\n","        # Check if all extracted features are empty\n","        if all(len(extracted_features[key]) == 0 for key in extracted_features):\n","            print(f\"Skipping row {idx} due to audio extraction issues.\")\n","            continue  # Optionally skip the row with extraction issues\n","\n","        # Extract CSV features\n","        mfcc_features = row.filter(like='mfcc').values.astype(np.float32)\n","        chroma_features = row.filter(like='chroma').values.astype(np.float32)\n","        rms_features = row.filter(like='rms').values.astype(np.float32)\n","        spectral_centroid_features = row.filter(like='spectral_centroid').values.astype(np.float32)\n","        spectral_bandwidth_features = row.filter(like='spectral_bandwidth').values.astype(np.float32)\n","        zero_crossing_rate_features = row.filter(like='zero_crossing_rate').values.astype(np.float32)\n","        tempo_features = row.filter(like='tempo').values.astype(np.float32)\n","\n","        # Add CSV features and extracted features to their respective lists\n","        features['mfcc'].append(mfcc_features)\n","        features['chroma'].append(chroma_features)\n","        features['rms'].append(rms_features)\n","        features['spectral_centroid'].append(spectral_centroid_features)\n","        features['spectral_bandwidth'].append(spectral_bandwidth_features)\n","        features['zero_crossing_rate'].append(zero_crossing_rate_features)\n","        features['tempo'].append(tempo_features)\n","        for key in extracted_features:\n","            features[key].append(extracted_features[key])\n","\n","        # Add label\n","        labels.append(row['label'])\n","\n","    # Convert features lists to NumPy arrays without padding/truncation\n","    for key in features:\n","        features[key] = np.array(features[key], dtype=object)  # Keep features as arrays of varying lengths\n","\n","    labels = np.array(labels)\n","\n","    return features, labels\n","\n","# Function to standardize features (without padding or truncation)\n","def standardize_features(features):\n","    standardized_features = {}\n","    scalers = {}\n","\n","    for key in features:\n","        standardized_features[key] = []\n","        scalers[key] = []\n","        for feature in features[key]:\n","            feature = feature.reshape(-1, 1)  # Reshape to 2D for StandardScaler\n","            scaler = StandardScaler()\n","            standardized_feature = scaler.fit_transform(feature).flatten()\n","            standardized_features[key].append(standardized_feature)\n","            scalers[key].append(scaler)  # Save scaler for future use (e.g., test data)\n","        standardized_features[key] = np.array(standardized_features[key], dtype=object)  # Keep as object array for varying lengths\n","\n","    return standardized_features, scalers"]},{"cell_type":"code","execution_count":null,"id":"825122af","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"825122af","outputId":"d006ca6a-f3ae-48bf-d0c5-68f134ae7f66"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-32cc003009b2>:35: UserWarning: PySoundFile failed. Trying audioread instead.\n","  y, sr = librosa.load(file_path, sr=sr)\n","/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n","\tDeprecated as of librosa version 0.10.0.\n","\tIt will be removed in librosa version 1.0.\n","  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"]},{"output_type":"stream","name":"stdout","text":["Error processing audio file /content/drive/MyDrive/Colab Notebooks/GTZAN/genres_original/jazz/jazz.00054.wav: \n","Skipping row 554 due to audio extraction issues.\n"]}],"source":["features, labels = process_dataset(df_30_sec, audio_directory)\n","\n","# Standardize features before splitting\n","standardized_features, scalers = standardize_features(features)\n","\n","# Split dataset into training, validation, and test sets\n","train_features = {}\n","val_features = {}\n","test_features = {}\n","# Create a combined list of feature dictionaries and labels for consistent splitting\n","combined_data = list(zip([{key: features[key][i] for key in features} for i in range(len(labels))], labels))\n","\n","# Split combined data\n","train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42, stratify=labels)\n","train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42, stratify=[label for _, label in train_data])  # 0.25 * 0.8 = 0.2 for validation set\n","\n","# Separate features and labels after splitting\n","for key in features:\n","    train_features[key] = [data[0][key] for data in train_data]\n","    val_features[key] = [data[0][key] for data in val_data]\n","    test_features[key] = [data[0][key] for data in test_data]\n","\n","train_labels = np.array([data[1] for data in train_data])\n","val_labels = np.array([data[1] for data in val_data])\n","test_labels = np.array([data[1] for data in test_data])\n"]},{"cell_type":"code","source":["SEED = 42\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","random.seed(SEED)\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","\n","# Encode labels as integers\n","label_encoder = LabelEncoder()\n","labels = label_encoder.fit_transform(labels)\n","train_labels = label_encoder.transform(train_labels)\n","val_labels = label_encoder.transform(val_labels)\n","test_labels = label_encoder.transform(test_labels)"],"metadata":{"id":"UISn8IP0NANm"},"id":"UISn8IP0NANm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data"],"metadata":{"id":"DmVnPUS-RulF"},"id":"DmVnPUS-RulF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"hI4sZbDCMzMP","metadata":{"id":"hI4sZbDCMzMP"},"outputs":[],"source":["# VGG-ish Model Implementation\n","import tensorflow_hub as hub\n","import numpy as np\n","\n","classes = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n","\n","# Load the model.\n","model = hub.load('https://kaggle.com/models/google/vggish/frameworks/TensorFlow2/variations/vggish/versions/1')\n","# model = tf.saved_model.load('/kaggle/input/music-genre-classification-vggish-model/VGGish')\n","\n","# Run the model, check the output.\n","pred_3s = model([w[0]['waveform'] for w in train_data])\n","\n","print(pred_3s)\n"]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","print(classification_report(train_labels, np.argmax(pred_3s, axis=1)))"],"metadata":{"id":"VHAHLlTCb_CL"},"id":"VHAHLlTCb_CL","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"wA6gtd65MvxG","metadata":{"id":"wA6gtd65MvxG"},"outputs":[],"source":["# Plotting model loss and accuracy for 30-sec and 3-sec\n","import matplotlib.pyplot as plt\n","\n","def plot_losses(hist):\n","\n","  fig, axs = plt.subplots(1,2, figsize=(12, 4))\n","  axs[0].plot(hist.history['loss'])\n","  axs[0].plot(hist.history['val_loss'])\n","  axs[0].set_title('Model Loss')\n","  axs[0].set_ylabel('Loss')\n","  axs[0].set_xlabel('Epoch')\n","  axs[0].legend(['Train', 'Test'], loc='upper right')\n","\n","  axs[1].plot(hist.history['accuracy'])\n","  axs[1].plot(hist.history['val_accuracy'])\n","  axs[1].set_title('Model Accuracy')\n","  axs[1].set_ylabel('Accuracy')\n","  axs[1].set_xlabel('Epoch')\n","  axs[1].legend(['Train', 'Test'], loc='upper right')\n","  plt.show()\n","\n","# Plot for 30-sec model\n","plot_losses(history_30s)\n","\n","# Plot for 3-sec model\n","plot_losses(history_3s)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"https://github.com/kaito1122/Music-Genre-Classifier/blob/main/data_processing.ipynb","timestamp":1730058309835}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}